[i] Loading defender model: mistralai/Mistral-7B-Instruct-v0.2
[i] Loading attacker model: mistralai/Mistral-7B-Instruct-v0.2
`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|###3      | 1/3 [00:02<00:05,  2.70s/it]
Loading checkpoint shards:  67%|######6   | 2/3 [00:03<00:01,  1.31s/it]
Loading checkpoint shards: 100%|##########| 3/3 [00:03<00:00,  1.26it/s]
Loading checkpoint shards: 100%|##########| 3/3 [00:03<00:00,  1.07s/it]
Some parameters are on the meta device because they were offloaded to the cpu.
Device set to use cuda:0

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  3.10it/s]
Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  4.21it/s]
Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  3.52it/s]
Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  3.57it/s]
Some parameters are on the meta device because they were offloaded to the cpu.
Device set to use cuda:0

Records:   0%|          | 0/3 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Records:   0%|          | 0/3 [06:25<?, ?it/s]
Traceback (most recent call last):
  File "C:\Users\sriro\OneDrive - University of Massachusetts\Documents\Umass\690F- Trustworthy AI\June_Repo\proj-group-04\assignment-5\attack_defense_sim.py", line 279, in <module>
    main()
  File "C:\Users\sriro\OneDrive - University of Massachusetts\Documents\Umass\690F- Trustworthy AI\June_Repo\proj-group-04\assignment-5\attack_defense_sim.py", line 180, in main
    d_follow_out = defender_pipe(follow_prompt, max_new_tokens=args.max_new_tokens, do_sample=False)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\pipelines\text_generation.py", line 332, in __call__
    return super().__call__(text_inputs, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\pipelines\base.py", line 1467, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\pipelines\base.py", line 1474, in run_single
    model_outputs = self.forward(model_inputs, **forward_params)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\pipelines\base.py", line 1374, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\pipelines\text_generation.py", line 432, in _forward
    output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\generation\utils.py", line 2564, in generate
    result = decoding_method(
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\generation\utils.py", line 2784, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\accelerate\hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\utils\generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\models\mistral\modeling_mistral.py", line 433, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\utils\generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\models\mistral\modeling_mistral.py", line 369, in forward
    hidden_states = decoder_layer(
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\accelerate\hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\utils\deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\models\mistral\modeling_mistral.py", line 231, in forward
    hidden_states, _ = self.self_attn(
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\accelerate\hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\utils\deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\models\mistral\modeling_mistral.py", line 169, in forward
    attn_output, attn_weights = attention_interface(
  File "C:\Users\sriro\.conda\envs\airgap-agent\lib\site-packages\transformers\integrations\sdpa_attention.py", line 96, in sdpa_attention_forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.67 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 8.74 GiB is allocated by PyTorch, and 537.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
